{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "90aef008-bd0a-4d78-a719-90745a80e0df",
   "metadata": {},
   "source": [
    "# **Mario**\n",
    "### This project aims to crete a Mario Agent that will beat the entirity of Super Mario Bros using Deep Reinforcement Learning and Double Deep Q-Networks\n",
    "\n",
    "**Eve Collier\\\n",
    "AI II - Spring 2025\\\n",
    "Final Project**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e8941f-0ec9-48c2-a565-6df78b7c6c43",
   "metadata": {},
   "source": [
    "![Mario](https://media3.giphy.com/media/v1.Y2lkPTc5MGI3NjExdmFvcTg0dDBnbnQ1b3BjbzlnemJnbGVrdm02a29pbDF4a3ExcGF2eiZlcD12MV9pbnRlcm5hbF9naWZfYnlfaWQmY3Q9Zw/DqqHabAaTHRII/giphy.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7a8897-f39b-4fd4-be92-11ce167dddf0",
   "metadata": {},
   "source": [
    "First, we need to install the game and import some stuff:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc5211c-5dd9-4cfd-b739-3e1cef6f4569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym_super_mario_bros==7.3.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (7.3.0)\n",
      "Requirement already satisfied: nes_py in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (8.2.1)\n",
      "Requirement already satisfied: gym>=0.17.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nes_py) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.18.5 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nes_py) (1.23.5)\n",
      "Requirement already satisfied: pyglet<=1.5.21,>=1.4.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nes_py) (1.5.21)\n",
      "Requirement already satisfied: tqdm>=4.48.2 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from nes_py) (4.67.1)\n",
      "Requirement already satisfied: cloudpickle>=1.2.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gym>=0.17.2->nes_py) (3.1.1)\n",
      "Requirement already satisfied: importlib_metadata>=4.8.0 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gym>=0.17.2->nes_py) (8.5.0)\n",
      "Requirement already satisfied: gym_notices>=0.0.4 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from gym>=0.17.2->nes_py) (0.0.8)\n",
      "Requirement already satisfied: zipp>=3.20 in /Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages (from importlib_metadata>=4.8.0->gym>=0.17.2->nes_py) (3.20.1)\n"
     ]
    }
   ],
   "source": [
    "!pip install gym_super_mario_bros==7.3.0 nes_py\n",
    "import gym_super_mario_bros\n",
    "import nes_py\n",
    "from nes_py.wrappers import JoypadSpace # Wrap the game \n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT # simplify what a Mario agent can do, 256 actions it can do otherwise\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import cv2\n",
    "import random\n",
    "from collections import deque, namedtuple\n",
    "\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Define Mario'sExperience\n",
    "Experience = namedtuple('Experience', \n",
    "                       ('state', 'action', 'next_state', 'reward', 'done'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a89a15b1-0cdf-4380-91ef-6ae116d3a8d1",
   "metadata": {},
   "source": [
    "Cool.\\\n",
    "Now, setup the game (AKA our environment):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e4437065-3bca-45cb-82df-fb6f4a5fe92a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym_super_mario_bros.make('SuperMarioBros-v0',apply_api_compatibility=True, render_mode = 'human')#,render_mode=\\\"human\\\n",
    "#env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "#env.observation_space.shape # gives us frame of game\n",
    "#env.action_space # actions we can take (simplemovement actions)\n",
    "#env.reset() \n",
    "#nextState, reward, done, trunc, info = env.step(action=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dd23864-3ce0-4b4b-a7f0-c5f0ad651cdc",
   "metadata": {},
   "source": [
    "now we will initalize our Double Deep Q- Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1ac53115-1067-4f5b-a84a-6169737f5ae8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced Frame Stacker with pit detection\n",
    "class EnhancedFrameStacker:\n",
    "    def __init__(self, stack_size=4):\n",
    "        self.stack_size = stack_size\n",
    "        self.frames = deque(maxlen=stack_size)\n",
    "        \n",
    "    def _detect_pits(self, frame):\n",
    "        #Detect pits by looking for sudden drops\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        resized = cv2.resize(gray, (84, 84))\n",
    "        edges = cv2.Canny(resized, 50, 150)\n",
    "        pit_mask = np.zeros_like(edges)\n",
    "        pit_mask[-20:, :] = edges[-20:, :]  # Look at bottom of screen\n",
    "        return (pit_mask > 0).astype(np.float32)\n",
    "    \n",
    "    def preprocess(self, frame):\n",
    "        if isinstance(frame, tuple):\n",
    "            frame = frame[0]\n",
    "\n",
    "        # First resize the frame to fix dimension error\n",
    "        frame = cv2.resize(frame, (84, 84), interpolation=cv2.INTER_AREA)\n",
    "            \n",
    "        # Base grayscale frame\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        #resized = cv2.resize(gray, (84, 84))\n",
    "        normalized = gray / 255\n",
    "        \n",
    "        # Object detection channels\n",
    "        #enemy_mask = self._detect_enemies(frame)\n",
    "        #obstacle_mask = self._detect_obstacles(frame)\n",
    "        enemy_mask = cv2.inRange(frame, (200, 0, 0), (255, 50, 50)) / 255.0\n",
    "        obstacle_mask = cv2.inRange(frame, (100, 60, 0), (180, 120, 80)) / 255.0\n",
    "        #pit_mask = self._detect_pits(frame)\n",
    "        # Pit detection with morphological operations\n",
    "        pit_mask = np.zeros_like(gray)\n",
    "        pit_mask[-20:, :] = gray[-20:, :] < 50  # Dark areas at bottom\n",
    "        \n",
    "        # Stack all channels\n",
    "        processed = np.stack([normalized, enemy_mask, obstacle_mask, pit_mask], axis=0)\n",
    "        return processed.astype(np.float32)\n",
    "    \n",
    "    def _detect_enemies(self, frame):\n",
    "        lower_red = np.array([200, 0, 0])\n",
    "        upper_red = np.array([255, 50, 50])\n",
    "        mask = cv2.inRange(frame, lower_red, upper_red)\n",
    "        return (cv2.resize(mask, (84, 84)) > 0).astype(np.float32)\n",
    "    \n",
    "    def _detect_obstacles(self, frame):\n",
    "        lower_brown = np.array([100, 60, 0])\n",
    "        upper_brown = np.array([180, 120, 80])\n",
    "        lower_gold = np.array([200, 180, 50])\n",
    "        upper_gold = np.array([255, 220, 150])\n",
    "        mask = cv2.inRange(frame, lower_brown, upper_brown) | cv2.inRange(frame, lower_gold, upper_gold)\n",
    "        return (cv2.resize(mask, (84, 84)) > 0).astype(np.float32)\n",
    "    \n",
    "    def reset(self, frame):\n",
    "        self.frames.clear()\n",
    "        frame = self.preprocess(frame)\n",
    "        for _ in range(self.stack_size):\n",
    "            self.frames.append(frame)\n",
    "        return self._get_stacked_frames()\n",
    "    \n",
    "    def append(self, frame):\n",
    "        frame = self.preprocess(frame)\n",
    "        self.frames.append(frame)\n",
    "        return self._get_stacked_frames()\n",
    "    \n",
    "    def _get_stacked_frames(self):\n",
    "        stacked = np.stack(self.frames, axis=0)  # Shape: [stack_size, channels, height, width]\n",
    "        return torch.FloatTensor(stacked).to(device)  # Remove unsqueeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac14c209-9c30-45cf-9d81-bbceac5c12ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioDQN(nn.Module):\n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "    \n",
    "    def __init__(self, input_channels, output_dim):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Convolutional layers - expects 4 channels per frame\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "    \n",
    "         # Calculate the output size of convolutional layers\n",
    "        with torch.no_grad():\n",
    "            # Create dummy input to determine conv output size\n",
    "            dummy_input = torch.zeros(1, input_channels, 84, 84).to(device)\n",
    "            conv_out = self.conv(dummy_input)\n",
    "            self.conv_out_size = conv_out.shape[1] \n",
    "        \n",
    "        # Value stream\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(self.conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        # Advantage stream\n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(self.conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Expected input shape: [batch, stack*channels, h, w]\n",
    "        if x.dim() == 4:  # [batch*stack, channels, h, w]\n",
    "            pass  # Already in correct format\n",
    "        elif x.dim() == 5:  # [batch, stack, channels, h, w]\n",
    "            x = x.view(-1, x.size(2), x.size(3), x.size(4))  # Combine batch and stack\n",
    "        else:\n",
    "            raise ValueError(f\"Unexpected input dimension: {x.dim()}\")\n",
    "    \n",
    "        features = self.conv(x)\n",
    "        values = self.value_stream(features)\n",
    "        advantages = self.advantage_stream(features)\n",
    "    \n",
    "        return values + (advantages - advantages.mean(dim=1, keepdim=True))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6497dd1-10b2-4b7a-a7ce-13c1453c611d",
   "metadata": {},
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from collections import deque, namedtuple\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "# Device setup\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Experience tuple\n",
    "Experience = namedtuple('Experience', \n",
    "                       ('state', 'action', 'next_state', 'reward', 'done'))\n",
    "class FrameStacker:\n",
    "    #Handles frame stacking without gym wrappers\n",
    "    def __init__(self, stack_size=4):\n",
    "        self.stack_size = stack_size\n",
    "        self.frames = deque(maxlen=stack_size)\n",
    "\n",
    "\n",
    "    def preprocess(self, frame):\n",
    "        # First ensure we have a numpy array\n",
    "        if isinstance(frame, tuple):\n",
    "            frame = frame[0]  # Take the first element if it's a tuple\n",
    "        #Convert to grayscale and resize\n",
    "        #frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        #frame = cv2.resize(frame, (84, 84))\n",
    "        #frame = frame / 255  # Normalize\n",
    "        # Convert to grayscale and resize\n",
    "        if len(frame.shape) == 3:  # If RGB image\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        frame = cv2.resize(frame, (84, 84))\n",
    "        frame = frame / 255.0  # Normalize\n",
    "        return frame\n",
    "        #return frame.astype(np.float32)  # Ensure float32 output\n",
    "    \n",
    "    def reset(self, frame):\n",
    "        #Initialize with the same first frame\n",
    "        self.frames.clear()\n",
    "        frame = self.preprocess(frame)\n",
    "        frame = np.array(frame, dtype=np.float32) if not isinstance(frame, np.ndarray) else frame.astype(np.float32)\n",
    "        for _ in range(self.stack_size):\n",
    "            self.frames.append(frame)\n",
    "            #self.frames.append(frame.astype(np.float32))  # Explicit type conversion error otherwise\n",
    "        return self._get_stacked_frames()\n",
    "    \n",
    "    #def append(self, frame):\n",
    "        #self.frames.append(frame)\n",
    "        #return self._get_stacked_frames()\n",
    "        \n",
    "\n",
    "    def _get_stacked_frames(self):\n",
    "        # Convert to numpy array with explicit float32 dtype\n",
    "        stacked = np.array(self.frames, dtype=np.float32)\n",
    "        return torch.FloatTensor(stacked).unsqueeze(0).to(device)\n",
    "\n",
    "    def append(self, frame):\n",
    "        frame = self.preprocess(frame)\n",
    "        self.frames.append(frame.astype(np.float32))  # Explicit type conversion\n",
    "        return self._get_stacked_frames()\n",
    "    \n",
    "    #def _get_stacked_frames(self):\n",
    "        #stacked = np.stack(self.frames, axis=0)\n",
    "        #return torch.FloatTensor(stacked).unsqueeze(0).to(device)\n",
    "\n",
    "class EnhancedFrameStacker(FrameStacker):\n",
    "    def preprocess(self, frame):\n",
    "        \"\"\"Enhanced preprocessing with object detection\"\"\"\n",
    "        if isinstance(frame, tuple):\n",
    "            frame = frame[0]\n",
    "            \n",
    "        # Convert to grayscale base frame\n",
    "        gray = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        resized = cv2.resize(gray, (84, 84))\n",
    "        normalized = resized / 255.0\n",
    "        \n",
    "        # Create object detection channels\n",
    "        enemy_mask = self._detect_enemies(frame)\n",
    "        obstacle_mask = self._detect_obstacles(frame)\n",
    "        \n",
    "        # Stack channels (original + object detection)\n",
    "        processed = np.stack([normalized, enemy_mask, obstacle_mask], axis=0)\n",
    "        return processed.astype(np.float32)\n",
    "\n",
    "    def _detect_enemies(self, frame):\n",
    "        \"\"\"Create binary mask of enemies using color thresholding\"\"\"\n",
    "        lower_red = np.array([200, 0, 0])\n",
    "        upper_red = np.array([255, 50, 50])\n",
    "        mask = cv2.inRange(frame, lower_red, upper_red)\n",
    "        return (cv2.resize(mask, (84, 84)) > 0).astype(np.float32)\n",
    "\n",
    "    def _detect_obstacles(self, frame):\n",
    "        \"\"\"Create binary mask of pipes/blocks using color thresholding\"\"\"\n",
    "        lower_brown = np.array([100, 60, 0])\n",
    "        upper_brown = np.array([180, 120, 80])\n",
    "        mask = cv2.inRange(frame, lower_brown, upper_brown)\n",
    "        return (cv2.resize(mask, (84, 84)) > 0).astype(np.float32)\n",
    "\n",
    "class MarioDQN(nn.Module):\n",
    "    \"\"\"Combined Double DQN Network\"\"\"\n",
    "    def __init__(self, input_channels, output_dim):\n",
    "        super().__init__()\n",
    "        self.output_dim = output_dim\n",
    "        \n",
    "        # Shared convolutional layers\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_channels, 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten()\n",
    "        )\n",
    "        \n",
    "        # Get conv output size\n",
    "        with torch.no_grad():\n",
    "            dummy = torch.zeros(1, input_channels, 84, 84)\n",
    "            self.conv_out = self.conv(dummy).shape[1]\n",
    "        \n",
    "        # Dueling DQN branches (optional improvement)\n",
    "        self.value_stream = nn.Sequential(\n",
    "            nn.Linear(self.conv_out, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 1)\n",
    "        )\n",
    "        \n",
    "        self.advantage_stream = nn.Sequential(\n",
    "            nn.Linear(self.conv_out, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, output_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.conv(x)\n",
    "        values = self.value_stream(features)\n",
    "        advantages = self.advantage_stream(features)\n",
    "        return values + (advantages - advantages.mean(dim=1, keepdim=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e44eaa7-8f37-4ab8-97ac-8133f3a31124",
   "metadata": {},
   "source": [
    "Now we are ready to initalize our Mario agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "13d723d7-a46e-4427-8bc0-b185f188ebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MarioAgent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.stacker = EnhancedFrameStacker(stack_size=4)\n",
    "        self.num_actions = env.action_space.n  # Store number of actions\n",
    "        self.stuck_threshold = 50  # frames without x-position change\n",
    "        self.stuck_counter = 0\n",
    "        self.last_x_pos = 0\n",
    "        self.turn_actions = [0, 2]  # Left and Right in SIMPLE_MOVEMENT\n",
    "        \n",
    "        # Networks\n",
    "        self.policy_net = MarioDQN(4, self.num_actions).to(device)\n",
    "        self.target_net = MarioDQN(4, self.num_actions).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        # Training params\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 64\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.0001)\n",
    "        \n",
    "        # Exploration\n",
    "        self.eps_start = 1.0\n",
    "        self.eps_end = 0.01\n",
    "        self.eps_decay = 1000000\n",
    "        self.steps_done = 0\n",
    "        self.best_reward = -np.inf\n",
    "        \n",
    "        # Action mapping\n",
    "        self.jump_action = 5  # Jump right in SIMPLE_MOVEMENT\n",
    "        self.lives = 3\n",
    "        self.current_world = 1\n",
    "        self.current_stage = 1\n",
    "\n",
    "    def _is_stuck(self, current_x_pos):\n",
    "        if abs(current_x_pos - self.last_x_pos) < 2:  # Minimal movement\n",
    "            self.stuck_counter += 1\n",
    "        else:\n",
    "            self.stuck_counter = 0\n",
    "        self.last_x_pos = current_x_pos\n",
    "        return self.stuck_counter > self.stuck_threshold\n",
    "\n",
    "    def _should_jump(self, state):\n",
    "        \"\"\"Determine if Mario should jump based on immediate dangers\"\"\"\n",
    "        state_np = state.cpu().numpy()[0]  # Shape: [channels, height, width]\n",
    "        \n",
    "        # Channel indices\n",
    "        ENEMY_CHANNEL = 1\n",
    "        OBSTACLE_CHANNEL = 2\n",
    "        PIT_CHANNEL = 3\n",
    "        \n",
    "        # Danger zones - ensure they're within array bounds\n",
    "        height, width = state_np.shape[1], state_np.shape[2]\n",
    "        FRONT_RANGE = slice(max(0, height-20), height)  # Bottom 20 pixels\n",
    "        ABOVE_RANGE = slice(0, min(20, height))         # Top 20 pixels\n",
    "        WIDTH_RANGE = slice(max(0, width//2-2), min(width//2+2, width))  # Center 4 pixels\n",
    "        \n",
    "        # Check dangers safely\n",
    "        try:\n",
    "            enemy_in_front = state_np[ENEMY_CHANNEL, FRONT_RANGE, WIDTH_RANGE].max() > 0.5\n",
    "            block_above = state_np[OBSTACLE_CHANNEL, ABOVE_RANGE, WIDTH_RANGE].max() > 0.5\n",
    "            pit_ahead = state_np[PIT_CHANNEL, FRONT_RANGE, WIDTH_RANGE].max() > 0.5\n",
    "        except ValueError:  # If any slice is empty\n",
    "            return False\n",
    "            \n",
    "        return enemy_in_front or block_above or pit_ahead\n",
    "\n",
    "\n",
    "    def select_action(self, state, info):\n",
    "    # Ensure state has correct dimensions [1, stack_size, channels, h, w]\n",
    "        if state.dim() == 4:\n",
    "            state = state.unsqueeze(0)  # Add batch dimension if missing\n",
    "    \n",
    "        # Immediate danger response\n",
    "        if self._should_jump(state):\n",
    "            return torch.tensor([[self.jump_action]], device=device, dtype=torch.long)\n",
    "    \n",
    "        # Force turn-around when stuck\n",
    "        if self._is_stuck(info['x_pos']):  # Use current_info instead of info\n",
    "            self.stuck_counter = 0\n",
    "            return torch.tensor([[random.choice(self.turn_actions)]], \n",
    "                              device=device, dtype=torch.long)\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\n",
    "            np.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "    \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.num_actions)]], \n",
    "                              device=device, dtype=torch.long)\n",
    "    \n",
    "    \n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample batch from memory\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, next_states, rewards, dones = zip(*batch)\n",
    "    \n",
    "        ## Convert to tensors with proper shapes\n",
    "        state_batch = torch.stack([s for s in states])  # [batch, stack, channels, h, w]\n",
    "        state_batch = state_batch.view(-1, 4, 84, 84)  # [batch*stack, channels, h, w]\n",
    "    \n",
    "        # Handle actions\n",
    "        action_batch = torch.cat(actions)  # [batch]\n",
    "    \n",
    "        # Handle next states\n",
    "        non_final_mask = torch.tensor(\n",
    "            [s is not None for s in next_states],\n",
    "            device=device, dtype=torch.bool\n",
    "        )\n",
    "        non_final_next_states = torch.stack([s for s in next_states if s is not None])\n",
    "        if non_final_next_states.dim() == 4:  # [batch, stack, channels, h, w]\n",
    "            non_final_next_states = non_final_next_states.view(-1, 4, 84, 84)\n",
    "        reward_batch = torch.cat(rewards)  # [batch_size]\n",
    "        done_batch = torch.tensor(dones, dtype=torch.float32, device=device)  # [batch_size]\n",
    "    \n",
    "        # Compute Q values\n",
    "        q_values = self.policy_net(state_batch)  # [batch_size*stack_size, num_actions]\n",
    "    \n",
    "        # Reshape and process Q-values\n",
    "        q_values = q_values.view(self.batch_size, -1, self.num_actions)  # [batch_size, stack_size, num_actions]\n",
    "        q_values = q_values.mean(dim=1)  # Average across stacked frames [batch_size, num_actions]\n",
    "    \n",
    "        # Gather the Q-values for taken actions\n",
    "        state_action_values = q_values.gather(1, action_batch.view(-1, 1))  # [batch_size, 1]\n",
    "    \n",
    "        # Compute expected Q values\n",
    "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "    \n",
    "        # Handle non-final next states\n",
    "        non_final_mask = torch.tensor(\n",
    "            [s is not None for s in next_states],\n",
    "            device=device, dtype=torch.bool\n",
    "        )\n",
    "    \n",
    "        if non_final_mask.any():\n",
    "            non_final_next_states = torch.cat([s.unsqueeze(0) for s in next_states if s is not None])\n",
    "            non_final_next_states = non_final_next_states.view(-1, 4, 84, 84)\n",
    "        \n",
    "            with torch.no_grad():\n",
    "                next_q_values = self.target_net(non_final_next_states)\n",
    "                next_q_values = next_q_values.view(-1, 4, self.num_actions).max(dim=2)[0]  # Max over actions\n",
    "                next_state_values[non_final_mask] = next_q_values.mean(dim=1)  # Average over frames\n",
    "    \n",
    "        # Compute expected state-action values\n",
    "        expected_state_action_values = (next_state_values * self.gamma * (1 - done_batch)) + reward_batch\n",
    "    \n",
    "        # Compute loss\n",
    "        loss = nn.SmoothL1Loss()(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "    \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "    def _calculate_reward(self, reward, info, prev_info):\n",
    "        \"\"\"Enhanced reward calculation with all components properly defined\"\"\"\n",
    "        # Get values with defaults from prev_info\n",
    "        current_life = info.get('life', prev_info['life'])\n",
    "        current_x = info.get('x_pos', prev_info['x_pos'])\n",
    "        current_status = info.get('status', prev_info['status'])\n",
    "        current_score = info.get('score', prev_info['score'])\n",
    "        current_y = info.get('y_pos', prev_info['y_pos'])\n",
    "    \n",
    "        # Calculate reward components\n",
    "        life_penalty = -25 if current_life < prev_info['life'] else 0\n",
    "        x_reward = (current_x - prev_info['x_pos']) * 0.2\n",
    "    \n",
    "        status_bonus = 0\n",
    "        if current_status == 'tall' and prev_info['status'] == 'small':\n",
    "            status_bonus = 10\n",
    "        elif current_status == 'fireball' and prev_info['status'] != 'fireball':\n",
    "            status_bonus = 15\n",
    "    \n",
    "        time_penalty = -0.1 if current_x == prev_info['x_pos'] else 0\n",
    "        danger_penalty = -1 if current_y < 50 else 0  # Simple danger detection\n",
    "        block_hit_bonus = 1 if current_score > prev_info['score'] else 0\n",
    "    \n",
    "        return (reward + x_reward + status_bonus + \n",
    "                time_penalty + danger_penalty + block_hit_bonus + \n",
    "                life_penalty)\n",
    "    \n",
    "       \n",
    "        \n",
    "    def train(self, total_episodes=10000):\n",
    "        for episode in range(total_episodes):\n",
    "            observation = self.env.reset()\n",
    "            state = self.stacker.reset(observation)  # Should return [stack, channels, h, w]\n",
    "            # Initialize with default values\n",
    "            prev_info = {\n",
    "                'life': 3,\n",
    "                'x_pos': 0,\n",
    "                'status': 'small',\n",
    "                'score': 0,\n",
    "                'y_pos': 0\n",
    "            }\n",
    "            info = prev_info\n",
    "        \n",
    "            # Reset environment\n",
    "            observation = self.env.reset()\n",
    "            state = self.stacker.reset(observation)\n",
    "            if state.dim() == 4:  # [stack, channels, h, w]\n",
    "                state = state.unsqueeze(0)  # Add batch dimension\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "        \n",
    "            while not done:\n",
    "                action = self.select_action(state, info)\n",
    "                # Take step and handle different info formats\n",
    "                step_result = self.env.step(action.item())\n",
    "            \n",
    "            \n",
    "                # Handle different return formats\n",
    "                if len(step_result) == 4:  # (obs, reward, done, info)\n",
    "                    next_frame, reward, done, info = step_result\n",
    "                elif len(step_result) == 5:  # (obs, reward, terminated, truncated, info)\n",
    "                    next_frame, reward, done, _, info = step_result\n",
    "                else:\n",
    "                    raise ValueError(f\"Unexpected step return format: {step_result}\")\n",
    "            \n",
    "            \n",
    "                # Calculate reward with safe dictionary access\n",
    "                reward = self._calculate_reward(reward, info, prev_info)\n",
    "            \n",
    "                # Store experience with properly shaped states\n",
    "                next_state = self.stacker.append(next_frame) if not done else None\n",
    "                self.memory.append(Experience(\n",
    "                    state.squeeze(0),  # Remove batch dim if present [stack, channels, h, w]\n",
    "                    action,\n",
    "                    next_state.squeeze(0) if next_state is not None else None,\n",
    "                    torch.FloatTensor([reward]).to(device),\n",
    "                    done\n",
    "                ))\n",
    "            \n",
    "                # Update state and info\n",
    "                state = next_state if next_state is not None else state\n",
    "                prev_info = {\n",
    "                    'life': info.get('life', prev_info['life']),\n",
    "                    'x_pos': info.get('x_pos', prev_info['x_pos']),\n",
    "                    'status': info.get('status', prev_info['status']),\n",
    "                    'score': info.get('score', prev_info['score']),\n",
    "                    'y_pos': info.get('y_pos', prev_info['y_pos'])\n",
    "                }\n",
    "                total_reward += reward\n",
    "            \n",
    "                # Train and update networks\n",
    "                if len(self.memory) > self.batch_size:\n",
    "                    self.optimize_model()\n",
    "                \n",
    "                if self.steps_done % 10000 == 0:\n",
    "                    self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            \n",
    "                self.steps_done += 1\n",
    "            \n",
    "            # Save best model\n",
    "            if total_reward > self.best_reward:\n",
    "                self.best_reward = total_reward\n",
    "                torch.save(self.policy_net.state_dict(), 'mario_best.pth')\n",
    "                \n",
    "            print(f\"Episode {episode}, Reward: {total_reward:.1f}, Lives: {info['life']}\")\n",
    "    \n",
    "    \n",
    "    def run_best_model(self):\n",
    "        \"\"\"Run the trained model with visualization\"\"\"\n",
    "        self.policy_net.load_state_dict(torch.load('mario_best.pth'))\n",
    "        self.policy_net.eval()\n",
    "        \n",
    "        state = self.stacker.reset(self.env.reset())\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            with torch.no_grad():\n",
    "                action = self.policy_net(state).max(1)[1].view(1, 1)\n",
    "            \n",
    "            next_frame, reward, done, _, info = self.env.step(action.item())\n",
    "            state = self.stacker.append(next_frame) if not done else None\n",
    "            total_reward += reward\n",
    "        \n",
    "        print(f\"Final Reward: {total_reward}\")\n",
    "        print(f\"World {info['world']}-{info['stage']} {'Completed!' if info['flag_get'] else 'Failed'}\")\n",
    "        print(f\"Lives remaining: {info['life']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efaa3df1-b30b-40b5-8ed3-14d819eca6ac",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "#!pip install gym_super_mario_bros==7.3.0 nes_py\n",
    "import gym_super_mario_bros\n",
    "import nes_py\n",
    "from nes_py.wrappers import JoypadSpace # Wrap the game \n",
    "from gym_super_mario_bros.actions import SIMPLE_MOVEMENT # simplify what a Mario agent can do, 256 actions it can do otherwise\n",
    "\n",
    "class MarioAgent:\n",
    "    def __init__(self, env, stack_frames=4):\n",
    "        self.env = env\n",
    "        self.stack_size = stack_frames\n",
    "        self.stacker = FrameStacker(stack_size=4)\n",
    "\n",
    "        \n",
    "        # Double DQN setup\n",
    "        self.policy_net = MarioDQN(4, env.action_space.n).to(device)\n",
    "        self.target_net = MarioDQN(4, env.action_space.n).to(device)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        # Training parameters\n",
    "        self.gamma = 0.99\n",
    "        self.batch_size = 32\n",
    "        self.memory = deque(maxlen=100000)\n",
    "        self.optimizer = optim.Adam(self.policy_net.parameters(), lr=0.00025)\n",
    "        \n",
    "        # Exploration\n",
    "        self.eps_start = 1.0\n",
    "        self.eps_end = 0.01\n",
    "        self.eps_decay = 500000\n",
    "        self.steps_done = 0\n",
    "        self.best_reward = -np.inf\n",
    "\n",
    "    def select_action(self, state):\n",
    "        sample = random.random()\n",
    "        eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\n",
    "            np.exp(-1. * self.steps_done / self.eps_decay)\n",
    "        self.steps_done += 1\n",
    "\n",
    "        # Convert state tensor to numpy for processing\n",
    "        state_np = state.cpu().numpy()[0]  # Shape: [channels, height, width]\n",
    "    \n",
    "        # Define danger zones (adjust these areas as needed)\n",
    "        front_zone = state_np[1, -10:, 40:44]  # Bottom center (immediate front)\n",
    "        above_zone = state_np[2, :20, 40:44]   # Top center (above Mario)\n",
    "    \n",
    "        # Check for enemies (channel 1) or obstacles (channel 2)\n",
    "        enemy_in_front = (front_zone > 0.5).any()\n",
    "        block_above = (above_zone > 0.5).any()\n",
    "    \n",
    "        # Force jump if danger detected (override epsilon-greedy)\n",
    "        if enemy_in_front or block_above:\n",
    "            jump_action = 3\n",
    "            return torch.tensor([[jump_action]], device=device, dtype=torch.long)\n",
    "\n",
    "\n",
    "        \n",
    "        if sample > eps_threshold:\n",
    "            with torch.no_grad():\n",
    "                return self.policy_net(state).max(1)[1].view(1, 1)\n",
    "        else:\n",
    "            return torch.tensor([[random.randrange(self.env.action_space.n)]], \n",
    "                              device=device, dtype=torch.long)\n",
    "\n",
    "    def optimize_model(self):\n",
    "        if len(self.memory) < self.batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample a batch from memory\n",
    "        batch = random.sample(self.memory, self.batch_size)\n",
    "        states, actions, next_states, rewards, dones = zip(*batch)\n",
    "        \n",
    "        # Convert to tensors\n",
    "        state_batch = torch.cat(states)\n",
    "        action_batch = torch.cat(actions)\n",
    "        reward_batch = torch.cat(rewards)\n",
    "        \n",
    "        # Compute Q values for current states\n",
    "        state_action_values = self.policy_net(state_batch).gather(1, action_batch)\n",
    "        \n",
    "        # Compute expected Q values\n",
    "        next_state_values = torch.zeros(self.batch_size, device=device)\n",
    "        \n",
    "        # Find non-final states\n",
    "        non_final_mask = []\n",
    "        non_final_next_states = []\n",
    "        for i, state in enumerate(next_states):\n",
    "            if state is not None:\n",
    "                non_final_mask.append(True)\n",
    "                non_final_next_states.append(state)\n",
    "            else:\n",
    "                non_final_mask.append(False)\n",
    "        \n",
    "        non_final_mask = torch.tensor(non_final_mask, device=device)\n",
    "        if len(non_final_next_states) > 0:\n",
    "            non_final_next_states = torch.cat(non_final_next_states)\n",
    "            with torch.no_grad():\n",
    "                next_state_values[non_final_mask] = self.target_net(non_final_next_states).max(1)[0]\n",
    "        \n",
    "        expected_state_action_values = (next_state_values * self.gamma) + reward_batch\n",
    "        \n",
    "        # Compute loss\n",
    "        criterion = nn.SmoothL1Loss()\n",
    "        loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
    "        \n",
    "        # Optimize\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_value_(self.policy_net.parameters(), 100)\n",
    "        self.optimizer.step()\n",
    "\n",
    "\n",
    "    def train(self, total_episodes=1000):\n",
    "        \"\"\"Silent training with best model saving\"\"\"\n",
    "        for episode in range(total_episodes):\n",
    "            raw_frame = self.env.reset()\n",
    "            #frame = self._preprocess_frame(raw_frame)\n",
    "            state = self.stacker.reset(raw_frame)\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            \n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                next_raw_frame, reward, done, _, info = self.env.step(action.item())\n",
    "\n",
    "                total_reward += reward\n",
    "                reward = torch.FloatTensor([reward]).to(device)\n",
    "                \n",
    "                # Store experience\n",
    "                next_state = self.stacker.append(next_raw_frame) if not done else None\n",
    "                self.memory.append(Experience(state, action, next_state, reward, done))\n",
    "\n",
    "                state = next_state if next_state is not None else state                \n",
    "                # Train\n",
    "                self.optimize_model()\n",
    "                \n",
    "                \n",
    "                # total_reward += reward\n",
    "            \n",
    "            # Update target network\n",
    "            if episode % 10 == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            \n",
    "            # Save best model\n",
    "            if total_reward > self.best_reward:\n",
    "                self.best_reward = total_reward\n",
    "                torch.save(self.policy_net.state_dict(), 'mario_best.pth')\n",
    "                \n",
    "            eps_threshold = self.eps_end + (self.eps_start - self.eps_end) * \\\n",
    "                np.exp(-1. * self.steps_done / self.eps_decay)\n",
    "            print(f\"Episode {episode}, Reward: {total_reward}, Epsilon: {eps_threshold:.2f}\")\n",
    "\n",
    "    def run_best_model(self):\n",
    "        \"\"\"Run the best saved model with rendering\"\"\"\n",
    "        self.policy_net.load_state_dict(torch.load('mario_best.pth'))\n",
    "        self.policy_net.eval()\n",
    "        \n",
    "        state = self.stacker.reset(self._preprocess_frame(self.env.reset()))\n",
    "        done = False\n",
    "        total_reward = 0\n",
    "        \n",
    "        while not done:\n",
    "            self.env.render()\n",
    "            with torch.no_grad():\n",
    "                action = self.policy_net(state).max(1)[1].view(1, 1)\n",
    "            \n",
    "            next_frame, reward, done, _, info = self.env.step(action.item())\n",
    "            state = self.stacker.append(next_frame) if not done else None\n",
    "            total_reward += reward\n",
    "        \n",
    "        print(f\"Final Reward: {total_reward}\")\n",
    "        print(f\"Final Position: {info['x_pos']}\")\n",
    "        print(f\"World {info['world']}-{info['stage']} {'Completed!' if info['flag_get'] else 'Failed'}\")\n",
    "\n",
    "\n",
    "class EnhancedMarioAgent(MarioAgent):\n",
    "    def __init__(self, env, stack_frames=4):\n",
    "        # Use enhanced frame stacker\n",
    "        self.stacker = EnhancedFrameStacker(stack_size=stack_frames)\n",
    "        super().__init__(env, stack_frames)\n",
    "        \n",
    "        # Action mapping (adjust based on SIMPLE_MOVEMENT)\n",
    "        self.jump_action = 1  # Index of jump action in action space\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        # Danger detection first\n",
    "        if self._should_jump(state):\n",
    "            return torch.tensor([[self.jump_action]], device=device, dtype=torch.long)\n",
    "            \n",
    "        # Original epsilon-greedy selection\n",
    "        return super().select_action(state)\n",
    "        \n",
    "    def _should_jump(self, state):\n",
    "        \"\"\"Enhanced danger detection using object channels\"\"\"\n",
    "        state_np = state.cpu().numpy()[0]  # Shape: [channels, height, width]\n",
    "        \n",
    "        # Channel indices (0=grayscale, 1=enemies, 2=obstacles)\n",
    "        ENEMY_CHANNEL = 1\n",
    "        OBSTACLE_CHANNEL = 2\n",
    "        \n",
    "        # Danger zones (adjust based on Mario's position in frame)\n",
    "        FRONT_RANGE = slice(70, 84)  # Bottom 14 pixels\n",
    "        ABOVE_RANGE = slice(0, 20)   # Top 20 pixels\n",
    "        WIDTH_RANGE = slice(40, 44)  # Center 4 pixels\n",
    "        \n",
    "        # Check for immediate dangers\n",
    "        enemy_in_front = state_np[ENEMY_CHANNEL, FRONT_RANGE, WIDTH_RANGE].max() > 0.5\n",
    "        block_above = state_np[OBSTACLE_CHANNEL, ABOVE_RANGE, WIDTH_RANGE].max() > 0.5\n",
    "        \n",
    "        return enemy_in_front or block_above\n",
    "\n",
    "    def train(self, total_episodes=1000):\n",
    "        \"\"\"Enhanced training with strategic rewards\"\"\"\n",
    "        for episode in range(total_episodes):\n",
    "            state = self.stacker.reset(self.env.reset())\n",
    "            total_reward = 0\n",
    "            done = False\n",
    "            prev_info = None\n",
    "            \n",
    "            while not done:\n",
    "                action = self.select_action(state)\n",
    "                next_frame, reward, done, _, info = self.env.step(action.item())\n",
    "                \n",
    "                # Enhanced reward shaping\n",
    "                if prev_info is not None:\n",
    "                    reward += self._calculate_strategic_reward(info, prev_info)\n",
    "                \n",
    "                # Store experience\n",
    "                next_state = self.stacker.append(next_frame) if not done else None\n",
    "                self.memory.append(Experience(\n",
    "                    state, action, next_state, \n",
    "                    torch.FloatTensor([reward]).to(device),\n",
    "                    done\n",
    "                ))\n",
    "                \n",
    "                # Train\n",
    "                self.optimize_model()\n",
    "                \n",
    "                state = next_state if next_state is not None else state\n",
    "                prev_info = info\n",
    "                total_reward += reward\n",
    "            \n",
    "            # Periodic updates\n",
    "            if episode % 10 == 0:\n",
    "                self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "            \n",
    "            if total_reward > self.best_reward:\n",
    "                self.best_reward = total_reward\n",
    "                torch.save(self.policy_net.state_dict(), 'mario_best.pth')\n",
    "                \n",
    "            print(f\"Episode {episode}, Reward: {total_reward}\")\n",
    "    \n",
    "    def _calculate_strategic_reward(self, info, prev_info):\n",
    "        \"\"\"Additional rewards for strategic actions\"\"\"\n",
    "        reward = 0\n",
    "        \n",
    "        # Reward for hitting blocks from below (potential powerups)\n",
    "        if info['status'] != prev_info['status']:\n",
    "            reward += 2  # Got bigger\n",
    "        \n",
    "        # Penalty for being stuck\n",
    "        if info['x_pos'] == prev_info['x_pos']:\n",
    "            reward -= 0.1\n",
    "            \n",
    "        return reward\n",
    "\n",
    "# Initialize\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0',\n",
    "                               apply_api_compatibility=True,\n",
    "                               render_mode='human')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "agent = EnhancedMarioAgent(env)\n",
    "\n",
    "# Silent training\n",
    "print(\"Training silently...\")\n",
    "agent.train(total_episodes=5000)\n",
    "\n",
    "# Final output\n",
    "print(\"\\nTraining complete. Running best model:\")\n",
    "nextState, reward, done, trunc, info = env.step(0)  # Initial step to start\n",
    "agent.run_best_model()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f353591-89e2-44e6-8061-621b893305fb",
   "metadata": {},
   "source": [
    "Putting it all together- make appropriate function calls:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6af19db6-7b12-4b98-8c42-c74ea6f6fc5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gym/envs/registration.py:555: UserWarning: \u001b[33mWARN: The environment SuperMarioBros-v0 is out of date. You should consider upgrading to version `v3`.\u001b[0m\n",
      "  logger.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/gym/envs/registration.py:627: UserWarning: \u001b[33mWARN: The environment creator metadata doesn't include `render_modes`, contains: ['render.modes', 'video.frames_per_second']\u001b[0m\n",
      "  logger.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training silently...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 10\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Silent training\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining silently...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_episodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# Final output\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining complete. Running best model:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[5], line 256\u001b[0m, in \u001b[0;36mMarioAgent.train\u001b[0;34m(self, total_episodes)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[38;5;66;03m# Train and update networks\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmemory) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size:\n\u001b[0;32m--> 256\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msteps_done \u001b[38;5;241m%\u001b[39m \u001b[38;5;241m10000\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    259\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_net\u001b[38;5;241m.\u001b[39mload_state_dict(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpolicy_net\u001b[38;5;241m.\u001b[39mstate_dict())\n",
      "Cell \u001b[0;32mIn[5], line 148\u001b[0m, in \u001b[0;36mMarioAgent.optimize_model\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    145\u001b[0m non_final_next_states \u001b[38;5;241m=\u001b[39m non_final_next_states\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m84\u001b[39m, \u001b[38;5;241m84\u001b[39m)\n\u001b[1;32m    147\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 148\u001b[0m     next_q_values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_net\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnon_final_next_states\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    149\u001b[0m     next_q_values \u001b[38;5;241m=\u001b[39m next_q_values\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m4\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_actions)\u001b[38;5;241m.\u001b[39mmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)[\u001b[38;5;241m0\u001b[39m]  \u001b[38;5;66;03m# Max over actions\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     next_state_values[non_final_mask] \u001b[38;5;241m=\u001b[39m next_q_values\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)  \u001b[38;5;66;03m# Average over frames\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 51\u001b[0m, in \u001b[0;36mMarioDQN.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnexpected input dimension: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mdim()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvalue_stream(features)\n\u001b[1;32m     53\u001b[0m advantages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39madvantage_stream(features)\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Initialize\n",
    "env = gym_super_mario_bros.make('SuperMarioBros-v0',\n",
    "                               apply_api_compatibility=True,\n",
    "                               render_mode='human')\n",
    "env = JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "agent = MarioAgent(env)\n",
    "\n",
    "# Silent training\n",
    "print(\"Training silently...\")\n",
    "agent.train(total_episodes=5000)\n",
    "\n",
    "# Final output\n",
    "print(\"\\nTraining complete. Running best model:\")\n",
    "nextState, reward, done, trunc, info = env.step(0)  # Initial step to start\n",
    "agent.run_best_model()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f973b5c9-7e42-4f03-a7ec-9e6f1ca246bf",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Flag to keep track of when to restart the game. Once game is done, it must be started\n",
    "#done = True\n",
    "# Loop thru all frames of game:\n",
    "#for step in range (100000):\n",
    "    # Are we done?\n",
    "    #if done:\n",
    "        #env.reset() # Restart for the new game\n",
    "    # Do random actions and get info back:\n",
    "    #action = env.action_space.sample() # random action\n",
    "    #obs, reward, terminated, truncated , info = env.step(action) # 'Pressing a button', step allows to pass action into game\n",
    "    #done = terminated or truncated\n",
    "    #env.render() # Display the game on screen\n",
    "#end.close() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8950ed89-abe7-4fe3-b59b-e390d5d47419",
   "metadata": {},
   "source": [
    "The first thing we want to do is define our neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6062fa2f-6bf0-410f-bbb9-28bc555ca95f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2ceb3c1-b303-4c4b-bab3-843395cbc7bd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3a7b4dce-4e3b-459a-abf3-614cb3b37a72",
   "metadata": {},
   "source": [
    "# WORKS CITED: #\n",
    "@misc{gym-super-mario-bros,\n",
    "  author = {Christian Kauten},\n",
    "  howpublished = {GitHub},\n",
    "  title = {{S}uper {M}ario {B}ros for {O}pen{AI} {G}ym},\n",
    "  URL = {https://github.com/Kautenja/gym-super-mario-bros},\n",
    "  year = {2018},\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335bbd35-531d-46a2-a095-ac23f94d4708",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2035c3-ae10-4d7e-9765-86c61b07f74f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
